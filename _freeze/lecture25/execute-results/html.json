{
  "hash": "a3d54da66e40b341fc9a1a0a4c6a8410",
  "result": {
    "markdown": "# Lecture 25 - Nov 2, 2023\n\n## Summary\n\nIn this lecture, we start discussing how to evaluate the performance of\nalgorithms based on time complexity.\n\n## Last lecture\n\nBacktracking.\n\n## Today\n\nComplexity analysis - time.\n\n## Algorithm\n\nThe set of steps to solve a problem. For example, find a number in a list.\n\n1. Linear search: Worst case scenario, the number we search for will be the last\n number. We will need to make $n$ comparisons.\n2. Binary search: We assume the array is ordered. We start looking in the\n  middle. If the number we are searching for is smaller, look left. Otherwise,\n  look right. With every comparison, you throw 1/2 of the remaining array. Worst\n  case is when you break the array several times until the remaining element\n  is just one. The number of comparisons is given by\n  $\\frac{n}{2^x} = 1 \\implies n = 2^x \\implies x = \\log_2(n)$.\n\nThe time it takes to run a program, depends on your computer power, nature of\ndata, compiler, programming language, and input size.\n\nHowever, the choice of your algorithm matters! If you get a faster computer for\na slow algorithm, as input size grows, computation will take more time than if\nyou have used a quicker algorithm.\n\n::: {.callout-note icon=false}\n## Example\n\n| Input size ($n$) | Linear search on fast computer | Binary search  on slow computer|\n|------------------|--------------------------------|--------------------------------|\n| 15               | 7 ns                           | 1000 ns                        |\n| 1000             | 500 ns                         | 2500 ns                        |\n| 16,000,000       | 8,000,000 ns                   | 6000 ns                        |\n:::\n\nWe are interested in predicting performance as input size grows.\n\nWe will use complexity analysis (time) to evaluate how fast an algorithm with\nrespect to input size.\n\n$T(n)$: run time estimate as a function of input size $n$ for one operation.\n$T(n) = 1$.\n\n::: {.callout-note icon=false}\n## Example\n\n::: {.columns}\n\n::: {.column width=\"47.5%\"}\n**Code snippet**\n\n```cpp\nint i = 3;\n```\n\n:::\n\n::: {.column width=\"5%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"47.5%\"}\n**Run-time estimate**\n\n$T(n) = 1$\n:::\n\n:::\n:::\n\n::: {.callout-note icon=false}\n## Example\n\n::: {.columns}\n\n::: {.column width=\"47.5%\"}\n**Code snippet**\n\n```cpp\nsum = 0;\nfor (int i = 0; i < n; i++) {\n  sum ++;\n}\n```\n\n:::\n\n::: {.column width=\"5%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"47.5%\"}\n**Run-time estimate**\n\n$T(n) = 3n + 2$\n\n$n$ times\n\n* $i < n$\n* $i++$\n* $sum++$\n\nPlus `sum = 0` and `int i = 0`.\n\n:::\n\n:::\n:::\n\n::: {.callout-note icon=false}\n## Example\n\n::: {.columns}\n\n::: {.column width=\"47.5%\"}\n**Code snippet**\n\nLinear search for $x$ in an array `a`\n```cpp\nfor (int i = 0; i < n; i++) {\n  if (a[i] == x) {\n    return true;\n  }\n  return false;\n}\n```\n\n:::\n\n::: {.column width=\"5%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"47.5%\"}\n**Run-time estimate**\n\nBest case: $T(n) = 4$.\n\nWorst case: $T(n) = 3n + 2$. The plus two is because of `int i = 0` and\n`return true`.\n\nAverage case: $T(n) = \\frac{3n + 2}{2} = 1 + frac{3}{2}n$.\n:::\n\n:::\n:::\n\nWhat is a better $T(n)$? Is it $3n + 2$ or $2n + 2$?\n\n* We **do not** care about constants or coefficients. A faster computer can make\n up this difference.\n* What truly makes a difference is the highest order term, as it dominates when\n  $n$ is large.\n* We use Big-O notation to represent $T(n)$.\n* $T(n) = O(g(n))$ if $T(n)$'s highest order term is $g(n)$ - disregarding\n  coefficients and constant.\n* $O(g(n))$ is the upper bound on $T(n)$.\n\n<img src=\"diagrams/lecture25-diagram1.svg\" alt=\"diagrams/lecture25-diagram1.svg\">\n\n::: {.callout-note icon=false}\n## Examples\n\n$T(n) = 1 = O(1)$\n\n$T(n) = 3n + 2 = O(n)$\n\n$T(n) = 3n^2 + n + 1 = O(n^2)$\n\n$T(n) = 10,000n^2 + 2 n \\log(n) + n^3 = O(n^3)$\n:::\n\nWhich term is more dominant?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# plot log(n) vs n vs nlog(n) bs 2^n vs n^2\nn <- seq(1, 10, 0.1)\n\ndf <- data.frame(\n  log_n = log(n),\n  n = n,\n  n_times_log_n = n * log(n),\n  n_pow_2 = n^2,\n  two_pow_n = 2^n\n)\n\ndf <- df %>% gather(fun, time, -n)\n\nggplot(df, aes(x = n, y = time, color = fun)) +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![](lecture25_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n$$\n\\log(n) < n < n \\log(n) < n^2 < n^3 < 2^n\n$$\n\n::: {.callout-note icon=false}\n## Example\n\nMatrix addition\n\n```cpp\nint n = 7;\nint A[n][n] = { ... };\nint B[n][n] = { ... };\nint C[n][n];\n\nfor (int i = 0; i < n; i++) {\n  for (int j = 0; i < n; i++) {\n    C[i][j] = A[i][j] + B[i][j];\n  }\n}\n```\n\nEach loop runs $n$ times.\n\nInner loop:\n\n```\nint j = 0 <- once\ntakes C1\n\nj < n, j++, C[i][j] = ... <- n times\ntakes C2\n```\n\n$T(n) = C_1 + C_2 n = O(n)$.\n\nOuter loop:\n\n```\nint i = 0 <- once\ntakes C3\n\ni < n, i++, for loop { ... } <- n times\ntakes C4    complexity O(n)\n```\n\n$T(n) = (C_4 + O(n))n + C_3 = O(n^2)$.\n\n$T(n)$ for matrix addition is $O(n^2)$.\n:::\n",
    "supporting": [
      "lecture25_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}